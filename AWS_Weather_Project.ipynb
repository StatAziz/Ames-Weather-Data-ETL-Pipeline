{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPv5Sph4GlpYy5BJRSBJOhZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XE2I0onztXuf"},"outputs":[],"source":["###### lamda function\n","\n","import json\n","import boto3\n","import urllib3\n","import datetime\n","\n","# REPLACE WITH YOUR DATA FIREHOSE NAME\n","FIREHOSE_NAME = 'PUT-S3-y2B3Y'\n","\n","def lambda_handler(event, context):\n","\n","    http = urllib3.PoolManager()\n","\n","    r = http.request(\"GET\", \"https://api.open-meteo.com/v1/forecast?latitude=42.0347&longitude=-93.6199&daily=temperature_2m_max,temperature_2m_min,daylight_duration,sunshine_duration,uv_index_max,precipitation_sum,rain_sum,wind_speed_10m_max&temperature_unit=fahrenheit&timezone=America%2FDenver&start_date=2024-03-01&end_date=2024-04-30\")\n","\n","    # turn it into a dictionary\n","    r_dict = json.loads(r.data.decode(encoding='utf-8', errors='strict'))\n","\n","    time_list = []\n","    for val in r_dict['daily']['time']:\n","        time_list.append(val)\n","\n","    temp_list1 = []\n","    for temp in r_dict['daily']['temperature_2m_max']:\n","        temp_list1.append(temp)\n","\n","    temp_list2 = []\n","    for temp in r_dict['daily']['temperature_2m_min']:\n","        temp_list2.append(temp)\n","\n","    duration_list1 = []\n","    for dur in r_dict['daily']['daylight_duration']:\n","        duration_list1.append(dur)\n","\n","    duration_list2 = []\n","    for dur in r_dict['daily']['sunshine_duration']:\n","        duration_list2.append(dur)\n","\n","    sum_list1 = []\n","    for total in r_dict['daily']['precipitation_sum']:\n","        sum_list1.append(total)\n","\n","    sum_list2 = []\n","    for total in r_dict['daily']['rain_sum']:\n","        sum_list2.append(total)\n","\n","    speed_list = []\n","    for speed in r_dict['daily']['wind_speed_10m_max']:\n","        speed_list.append(speed)\n","\n","    # extract pieces of the dictionary\n","    processed_dict = {}\n","\n","    # append to string running_msg\n","    running_msg = ''\n","    for i in range(len(time_list)):\n","        # construct each record\n","        processed_dict['latitude'] = r_dict['latitude']\n","        processed_dict['longitude'] = r_dict['longitude']\n","        processed_dict['time'] = time_list[i]\n","        processed_dict['temp_f'] = temp_list1[i]\n","        processed_dict['temp_f'] = temp_list2[i]\n","        processed_dict['row_ts'] = str(datetime.datetime.now())\n","        processed_dict['daylight_duration'] = duration_list1[i]\n","        processed_dict['sunshine_duration'] = duration_list2[i]\n","        processed_dict['precipitation_sum'] = sum_list1[i]\n","        processed_dict['rain_sum'] = sum_list2[i]\n","        processed_dict['wind_speed_10m_max'] = speed_list[i]\n","\n","    # add a newline to denote the end of a record\n","    # add each record to the running_msg\n","        running_msg += str(processed_dict) + '\\n'\n","\n","    # cast to string\n","    running_msg = str(running_msg)\n","    fh = boto3.client('firehose')\n","\n","    reply = fh.put_record_batch(\n","        DeliveryStreamName=FIREHOSE_NAME,\n","        Records = [\n","                 {'Data': running_msg}\n","    ]\n","    )\n","\n","    return reply\n","\n","\n","###### Glue Jobs\n","###### delete parquet table job\n","import sys\n","import json\n","import boto3\n","\n","# replace these with the names from your environment\n","BUCKET_TO_DEL = 'historical-weather-data-final-parquet-bucket'\n","DATABASE_TO_DEL = 'de_proj_database_hist_final'\n","TABLE_TO_DEL = 'historical_weather_data_final_parquet_tbl'\n","QUERY_OUTPUT_BUCKET = 's3://query-results-location-de-proj-3/'\n","# note: this script will only delete the parquet table, not bucket and database\n","\n","# delete all objects in the bucket\n","s3_client = boto3.client('s3')\n","\n","while True:\n","    objects = s3_client.list_objects(Bucket=BUCKET_TO_DEL)\n","    content = objects.get('Contents', [])\n","    if len(content) == 0:\n","        break\n","    for obj in content:\n","        s3_client.delete_object(Bucket=BUCKET_TO_DEL, Key=obj['Key'])\n","\n","\n","# drop the table too\n","client = boto3.client('athena')\n","\n","queryStart = client.start_query_execution(\n","    QueryString = f\"\"\"\n","    DROP TABLE IF EXISTS {DATABASE_TO_DEL}.{TABLE_TO_DEL};\n","    \"\"\",\n","    QueryExecutionContext = {\n","        'Database': f'{DATABASE_TO_DEL}'\n","    },\n","    ResultConfiguration = { 'OutputLocation': f'{QUERY_OUTPUT_BUCKET}'}\n",")\n","\n","# list of responses\n","resp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\n","\n","# get the response\n","response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# wait until query finishes\n","while response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\n","    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# if it fails, exit and give the Athena error message in the logs\n","if response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\n","    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])\n","\n","###### create parquet table job\n","\n","import sys\n","import boto3\n","\n","client = boto3.client('athena')\n","\n","SOURCE_TABLE_NAME = 'historical_weather_data_final' #the original table where crawler first stores data in athena\n","NEW_TABLE_NAME = 'historical_weather_data_final_parquet_tbl' # running this whole script will create this table in athena\n","NEW_TABLE_S3_BUCKET = 's3://historical-weather-data-final-parquet-bucket/' # you need to create this bucket manually in s3 beforehead\n","MY_DATABASE = 'de_proj_database_hist_final' # source table database\n","QUERY_RESULTS_S3_BUCKET = 's3://query-results-location-de-proj-3'  # no need to create fresh bucket for query results\n","\n","# Refresh the table\n","queryStart = client.start_query_execution(\n","    QueryString = f\"\"\"\n","    CREATE TABLE {NEW_TABLE_NAME} WITH\n","    (external_location='{NEW_TABLE_S3_BUCKET}',\n","    format='PARQUET',\n","    write_compression='SNAPPY',\n","    partitioned_by = ARRAY['time'])\n","    AS\n","\n","    SELECT\n","        latitude\n","        ,longitude\n","        ,temp_F\n","        ,(temp_F - 32) * (5.0/9.0) AS temp_C\n","        ,row_ts\n","        ,daylight_duration\n","        ,sunshine_duration\n","        ,precipitation_sum\n","        ,rain_sum\n","        ,wind_speed_10m_max\n","        ,time\n","    FROM \"{MY_DATABASE}\".\"{SOURCE_TABLE_NAME}\"\n","\n","    ;\n","    \"\"\",\n","    QueryExecutionContext = {\n","        'Database': f'{MY_DATABASE}'\n","    },\n","    ResultConfiguration = { 'OutputLocation': f'{QUERY_RESULTS_S3_BUCKET}'}\n",")\n","\n","# list of responses\n","resp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\n","\n","# get the response\n","response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# wait until query finishes\n","while response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\n","    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# if it fails, exit and give the Athena error message in the logs\n","if response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\n","    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])\n","\n","\n","###### data quality check job\n","\n","import sys\n","import awswrangler as wr\n","\n","# this check counts the number of NULL values in the response variable columns\n","# if any column contains NULL, the check returns a number > 0\n","NULL_DQ_CHECK = f\"\"\"\n","SELECT\n","\n","    SUM(CASE WHEN temp_f IS NULL THEN 1 ELSE 0 END) AS temp_f_nulls,\n","    SUM(CASE WHEN temp_c IS NULL THEN 1 ELSE 0 END) AS temp_c_nulls,\n","    SUM(CASE WHEN daylight_duration IS NULL THEN 1 ELSE 0 END) AS daylight_duration_nulls,\n","    SUM(CASE WHEN sunshine_duration IS NULL THEN 1 ELSE 0 END) AS sunshine_duration_nulls,\n","    SUM(CASE WHEN precipitation_sum IS NULL THEN 1 ELSE 0 END) AS precipitation_sum_nulls,\n","    SUM(CASE WHEN rain_sum IS NULL THEN 1 ELSE 0 END) AS rain_sum_nulls,\n","    SUM(CASE WHEN wind_speed_10m_max IS NULL THEN 1 ELSE 0 END) AS wind_speed_10m_max_nulls\n","\n","FROM \"de_proj_database_hist_final\".\"historical_weather_data_final_parquet_tbl\"\n",";\n","\"\"\"\n","\n","# run the quality check\n","df = wr.athena.read_sql_query(sql=NULL_DQ_CHECK, database=\"de_proj_database_hist_final\")\n","\n","# exit if we get a result > 0\n","# else, the check was successful\n","if df['temp_f_nulls'][0] > 0 or df['temp_c_nulls'][0] > 0 or df['daylight_duration_nulls'][0] > 0 or df['sunshine_duration_nulls'][0] > 0 or df['precipitation_sum_nulls'][0] > 0 or df['rain_sum_nulls'][0] > 0 or df['wind_speed_10m_max_nulls'][0] > 0 :\n","    sys.exit('Results returned. Quality check failed.')\n","else:\n","    print('Quality check passed.')\n","\n","\n","###### publish production table job\n","\n","import sys\n","import boto3\n","from datetime import datetime\n","\n","QUERY_RESULTS_BUCKET = 's3://query-results-location-de-proj-3/' # same old bucket, not necessary to create new one\n","MY_DATABASE = 'de_proj_database_hist_final'\n","SOURCE_PARQUET_TABLE_NAME = 'historical_weather_data_final_parquet_tbl'\n","NEW_PROD_PARQUET_TABLE_NAME = 'historical_weather_data_final_prod_tbl' # this whole query will create this table with datetime in athena\n","NEW_PROD_PARQUET_TABLE_S3_BUCKET = 's3://historical-weather-data-final-prod-bucket' # need to create this bucket in s3 manually before run\n","\n","# create a string with the current UTC datetime\n","# convert all special characters to underscores\n","# this will be used in the table name and in the bucket path in S3 where the table is stored\n","DATETIME_NOW_INT_STR = str(datetime.now()).replace('-', '_').replace(' ', '_').replace(':', '_').replace('.', '_')\n","\n","client = boto3.client('athena')\n","\n","# Refresh the table\n","queryStart = client.start_query_execution(\n","    QueryString = f\"\"\"\n","    CREATE TABLE {NEW_PROD_PARQUET_TABLE_NAME}_{DATETIME_NOW_INT_STR} WITH\n","    (external_location='{NEW_PROD_PARQUET_TABLE_S3_BUCKET}/{DATETIME_NOW_INT_STR}/',\n","    format='PARQUET',\n","    write_compression='SNAPPY',\n","    partitioned_by = ARRAY['time'])\n","    AS\n","\n","    SELECT\n","        *\n","    FROM \"{MY_DATABASE}\".\"{SOURCE_PARQUET_TABLE_NAME}\"\n","\n","    ;\n","    \"\"\",\n","    QueryExecutionContext = {\n","        'Database': f'{MY_DATABASE}'\n","    },\n","    ResultConfiguration = { 'OutputLocation': f'{QUERY_RESULTS_BUCKET}'}\n",")\n","\n","# list of responses\n","resp = [\"FAILED\", \"SUCCEEDED\", \"CANCELLED\"]\n","\n","# get the response\n","response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# wait until query finishes\n","while response[\"QueryExecution\"][\"Status\"][\"State\"] not in resp:\n","    response = client.get_query_execution(QueryExecutionId=queryStart[\"QueryExecutionId\"])\n","\n","# if it fails, exit and give the Athena error message in the logs\n","if response[\"QueryExecution\"][\"Status\"][\"State\"] == 'FAILED':\n","    sys.exit(response[\"QueryExecution\"][\"Status\"][\"StateChangeReason\"])\n","\n","\n","\n","############################################################# End ######################################\n","\n"]}]}